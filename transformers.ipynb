{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "356bf51d",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9da4e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import warnings\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# Optional imports - only import if available\n",
    "try:\n",
    "    import spacy\n",
    "except ImportError:\n",
    "    spacy = None\n",
    "\n",
    "try:\n",
    "    import GPUtil\n",
    "except ImportError:\n",
    "    GPUtil = None\n",
    "\n",
    "# Set to False to skip notebook execution (e.g. for debugging)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfbf016",
   "metadata": {},
   "source": [
    "# Useful Functions\n",
    "Some convenience helper functions used throughout the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4310884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_interactive_notebook():\n",
    "    return __name__ == \"__main__\"\n",
    "\n",
    "\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "\n",
    "\n",
    "def execute_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        fn(*args)\n",
    "\n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493ab884",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Structure  \n",
    "The encoder-decoder structure allows differences between the lengths of the input and output sequences, this avoids padding the sequences, which causes information loss and missing. This structure ensured both encoder and decoder can be used as a tool to embed in any algorithm and can be used in multimodal learning by changing different types of information into sequences, like changing images or audio files to its binary form. Besides, training the encoder and decoder separately can significantly cut the cost of training, making the whole system more efficient.  \n",
    "The encoder-decoder structure in the Transformers structure has two main sections, Self-Attention and Feed-Forward Neural Network, which will be followed up below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "234445a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    the encoder maps an input sequence of symbol representations \n",
    "    (x_1, ..., x_n) to a sequence of continuous representations, z=\n",
    "    (z_1, ..., z_n).\n",
    "    Encoder-Decoder architecture changes input sequences into vectors,\n",
    "    vice-versa.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"\"\"Take in and process masked src and target sequences.\"\"\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask) \n",
    "        # encode and decode\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask) \n",
    "        # sequences to vectors, vector lengths are the same\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask) \n",
    "        # vectors to sequences\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"\n",
    "        linear transformation, x * W^T + b, W as weight and b as bias, d_model is the size \n",
    "        of the input tensor, vocab is the size of the output tensor. for an input tensor like \n",
    "        (batch_size, seq_length, input_size), tensor[-1] would be d_model.\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__() \n",
    "        #make initialisation global\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Here we take the log of the partition function, log(exp(x_i) / sum(exp(x_j) for j in\n",
    "        range(len(x). This avoids data overflow.\n",
    "        \"\"\"\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050ada52",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "The encoder is a tool using for changing input sequences (discrete numbers) into tensors (continuous variables that can be easily processed by GPUs). Encoder processes sequence token by token and capture its relationship of relevant contents before and after the exact token to give a predicted vector (tensor). This means each token can be deducted from other token, they do not have to be saved in fixed length. Mathematical expression as follows: \n",
    "\n",
    "Changing from discrete quantities:$$(x_1, x_2, x_3...)$$to continuous function:\n",
    "$$f(x_1, x_2, x_3...)$$\n",
    "The encoder may ignore the detail of the whole input, imagine human reading books, people could memorize a general concept instead of every single word. The solution people came out nowadays will be explained later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0567de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Core encoder is a stack of N layers\"\"\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N) \n",
    "        # make \"N\" deep copies of layer\n",
    "        self.norm = nn.LayerNorm(layer.size) \n",
    "        # normalise every single layer\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"Pass the input (and mask) through each layer in turn.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask) \n",
    "            # define layers\n",
    "        return self.norm(x) \n",
    "        # normalise output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9a8ae5",
   "metadata": {},
   "source": [
    "## Decoder  \n",
    "The decoder follows same rule but functions an opposite way-it builds output sequence from the tensor generated by the encoder. To be specific, the decoder reads the tensor to find meaning of the input sequence and corresponding knowledge, which is embedded as tensors, in the pool to produce the answer. When generating the answer, decoder also reads the generated part to ensure the generation does not include any content obeys the causal sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e0cc0c",
   "metadata": {},
   "source": [
    "## Feed-Forward Neural Network  \n",
    "The FFN was designed to have a deeper and non-linear output for the model and process the contents token by token. The attention mechanism cannot capture any non-linear relation between contents.   This structure bases on the Multilayer Perceptron(MLP), expressed as:\n",
    "$$\n",
    "FFN(x)=W_2​\\cdot max(0,W_1​\\cdot x+b_1​)+b_2​\n",
    "$$\n",
    "In this equation, variable $x$ is the input token, $W_1$ and $b_1$ is the weight and bias of the first layer that maps the input matrices into a larger dimension, while $W_2$ and $b_2$ are the ones for the second layer that maps the dimension to the original one. The ReLU, defined as $f(x) = max(0, x)$, acts as a gate that performs deeper and non-linear features for the matrices. The processing by elements are parallel to significantly reduce the training time.  \n",
    "A more updated version is expressed as following:\n",
    "$$\n",
    "y=(x\\cdot W_u​)\\odot \\psi(x\\cdot W_v​)W_o​\n",
    "$$\n",
    "The $\\odot$ means multiplication by elements and bias could be removed as an engineering option.  \n",
    "For tensors with dimension $(B,T,d_{model})$, the amount of computation of the FFN has complexity $O(B\\cdot T\\cdot d_{model}\\cdot d_{ff})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9703359f",
   "metadata": {},
   "source": [
    "## Normalisation  \n",
    "  \n",
    "The Layer Normalisation is used for minimise the stability issues in activation distribution between batches and layers, making the training more stable. The normalisation is in feature dimensions and independent from batch size, therefore it is friendly to the small batch, longer sequence and online reasoning. \n",
    "  \n",
    "Layer Normalisation includes following content:\n",
    "  \n",
    "1. Calculating Mean and Square Roots:  \n",
    "$\\mu = \\frac{1}{D} \\sum_{i}^{D}x_{i}$  \n",
    "$\\sigma ^2 = \\frac{1}{D} \\sum_{i}^{D}(x_{i}-\\mu )^2$  \n",
    "Where D is the number for feature dimension, which is also the second parameter in the input tensor [S, D].  \n",
    "  \n",
    "2. Normalisation:  \n",
    "$\\hat{x} = \\frac{x-\\mu}{\\sqrt{\\sigma ^2+\\epsilon } }$  \n",
    "The calculation was element by element.  \n",
    "  \n",
    "3. Affine transformation:  \n",
    "However, the normalisation means any result from previous layer would be erased, therefore we need some adjustments to make sure the features could be either restored or learned from future training.  \n",
    "$y = \\gamma \\odot \\hat{x} + \\beta$  \n",
    "Where y is the output and $\\gamma$ is the gain, $\\beta$ is the bias.  \n",
    "  \n",
    "The structure in the original transformers is slightly different from what it is nowadays:  \n",
    "  \n",
    "In the original paper(Post-LN), LayerNorm after all calculation processes:  \n",
    "LN(x + SelfAttention(x))  \n",
    "LN(x + FeedForward(x))  \n",
    "\n",
    "In the nowadays models(Pre-LN), LayerNorm before applying Attention or other algorithms:  \n",
    "x = x + SelfAttention(LN(x))  \n",
    "x = x + FeedForward(LN(x))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53380dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Construct a layernorm module (See arXiv:1607.06450). The goal is to calculate mean and \n",
    "    stddev for every single sample with their own characteristic dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        \"\"\"\n",
    "        nn.Parameter save parameters as part of nn.Module, it is included in gradiant \n",
    "        calculation so it can be traced and updated during training sessions\n",
    "        \"\"\"\n",
    "        self.a_2 = nn.Parameter(torch.ones(features)) \n",
    "        # create tensors with all elements equal to 1 with size \"features\"\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Specific function to make convergence faster.\"\"\"\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * ((x - mean) / (std + self.eps)) + self.b_2 \n",
    "        # first term and last term made Affine Transformation, the term in the middle is normalisation term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da995bd",
   "metadata": {},
   "source": [
    "## Transformers / Attention Mechanism In Decoder Structure  \n",
    "\n",
    "The Transformer makes sequential encoding capable for parallel computing by introducing the Self-Attention mechanism: each Input Sequence Matrix (X) has a weight matrix(W) to calculate Query(Q), Key(K) and Value(V):\n",
    "$$\n",
    "Q = X\\cdot W_Q\\ ,\\  K = X\\cdot W_K\\ ,\\  V = X\\cdot W_V\n",
    "$$\n",
    "\n",
    "Each of the matrices has shape of ```[batch_size, seq_len, d_k]```.  \n",
    "In the Attention Mechanism, the Query gives the Attention which the token being processed needed to put on the other tokens in the pool to find the most relevant ones to generate the output.  \n",
    "The Key suggests the token being selected from the pool, the accuracy of the embedded tensor gives more accurate Attention Score, which means better response. The Value suggests the complete content with same label as the Value and the result from matrices' multiplication will be the final output.   \n",
    "The Attention Weight is calculated by:\n",
    "$$\n",
    "Attention(Q,K,V)=softmax(\\frac{Q\\cdot K^{T}}{\\sqrt{d_{K}} }) \\cdot V\n",
    "$$\n",
    "Where the term inside softmax function is the Attention Score. The softmax function acts like partition function we use in statistics (the weight acts like probability of a specific state and score is the overall probability). The Attention Score is the overall relevance between Q and K, while the Attention Weight is the one we used for specific calculation.  \n",
    "The encoder uses Attention the way same as what we described before. However, the Attention system is used differently in the decoder. The Causal Attention mechanism needs a matrix to achieve the Masked Self-Attention in the decoder, calculated by:\n",
    "$$\n",
    "MaskedAttention(Q,K,V) = softmax(\\frac{Q\\cdot K^{T}}{\\sqrt{d_{K}}} + M)\\cdot V\n",
    "$$so that the Query does not include anything not provided yet. To be specific, the Query of the decoder comes from its own output. The Key and Value still comes from input sequence and encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd12e5f3",
   "metadata": {},
   "source": [
    "## Generator Step  \n",
    "  \n",
    "This ia a standard standard linear + softmax generation step. The "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
